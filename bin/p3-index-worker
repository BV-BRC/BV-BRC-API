#!/usr/bin/env node  --max-old-space-size=4096
const config = require('../config')
const Queue = require('file-queue').Queue
const Defer = require('promised-io/promise').defer
const when = require('promised-io/promise').when
const All = require('promised-io/promise').all
const Path = require('path')
const fs = require('fs-extra')
const Request = require('request')
const JSONStream = require('JSONStream')
const es = require('event-stream')

const queueDirectory = config.get('queueDirectory')
// require('request').debug = true

// console.log(`Queue Directory: ${queueDirectory}`)

const queue = new Queue(queueDirectory, function (err) {
  if (err) {
    console.log(`error: ${err}`)
    return
  }
  processQueue(queue)
})

let completedHardCommit = false

const publicCores = ['taxonomy']
const IndexingCores = ['genome_feature', 'genome', 'sp_gene', 'pathway', 'genome_sequence', 'genome_amr', 'subsystem']

process.on('message', (msg) => {
  if (msg && msg.type === 'start') {
    if (timer) {
      cancelTimeout(timer)
      // delete timer;
      timer = null
    }

    processQueue(queue)
  }
})

let timer

function deleteDocs (core, genomeId) {
  const url = `${config.get('solr').url}/${core}/update?stream.body=<delete><query>genome_id:${genomeId}</query></delete>`

  Request.get(url, (err, res, body) => {
    if (err) {
      console.log(err)
      return
    }
    if (res.statusCode !== 200 || body.error) {
      console.log(body.error.msg)
      return
    }
    console.log(`Sent delete query successfully: ${url}`)
  })
}

function postDocs (filePath, type) {
  const def = new Defer()
  const url =
  `${config.get('solr').url}/${type}/update?wt=json&overwrite=true&commit=false`

  fs.createReadStream(filePath).pipe(
    Request.post(url, (err, res, bodyStr) => {
      const body = JSON.parse(bodyStr)

      if (err) {
        // if solr is not available
        if (err.code === 'ECONNREFUSED') {
          console.log(`[${(new Date()).toISOString()}] Solr is not available: ${err.message}`)
          // need to stop & notify
          process.exit()
        }
        // other errors reported in http response
        console.log(`Error POSTing to ${type} - ${err}`)
        def.reject(err)
        return def.promise
      }

      // http response is okay, but internal solr errors
      if (res.statusCode !== 200 || body.error) {
        def.reject(new Error(`${res.statusCode} ${res.statusMessage || ''} - ${body.error.msg}`))
        return def.promise
      }

      def.resolve(true)
    })
  )

  return def.promise
}

function processQueue (queue) {
  queue.length((err, length) => {
    if (length < 1) {
      if (!completedHardCommit) {
        console.log('Queue is Empty')
        completedHardCommit = true
        setTimeout(() => {
          processQueue(queue)
        }, 5000)
      } else {
        setTimeout(() => {
          processQueue(queue)
        }, 15000)
      }
    } else {
      completedHardCommit = false
      console.log(`Start processing queued items: ${length}`)

      queue.tpop((err, message, commit, rollback) => {
        if (err) {
          console.log(`Error popping message item. ${err}`)
          return
        }
        if (!message) {
          console.log(`No Metadata for: ${message} Discarding Message.`)
          if (commit) {
            commit()
          }
          processQueue(queue)
          return
        }

        try {
          console.log(`\nStart indexing ${message.id} User: ${message.user}`)

          const fileDefs = []
          const beforeProcess = (new Date()).getTime()

          const genomeId = fs.readJsonSync(message.files.genome.path)[0]['genome_id']

          Object.keys(message.files).forEach(core => {
            let files = message.files[core]
            if (!(files instanceof Array)) {
              files = [files]
            }
            files.forEach(file => {
              if (!file.path) { return }

              const def = new Defer()

              when(updateJSONStreamWithAccessControl(file.name, file.path, core, message.user), (res) => {
                if (res.status === 'skip') {
                  def.resolve(true)
                } else if (res.status === 'saved') {
                  when(postDocs(file.path, core), () => {
                    def.resolve(true)
                  }, (err) => {
                    console.error(`${(new Date()).toISOString()}: Error POSTing documents to SOLR core : ${core}, ${err}`)
                    def.reject(err)
                  })
                } else {
                  // Err
                  def.reject(res)
                }
              }, (err) => {
                def.reject(err)
              })

              fileDefs.push(def)
            })
          })

          when(All(fileDefs), () => {
            const afterProcess = (new Date()).getTime()
            console.log(`  ${(afterProcess - beforeProcess)}ms elapsed for genome (${genomeId})`)

            when(updateHistory(message.id, 'submitted', genomeId), () => {
              commit()
              processQueue(queue)
            }, (err) => {
              console.error(`${(new Date()).toISOString()}: Error in updating history: ${err} while processing ${message.id}`)
            })
          }, (err) => {
            console.error(`${(new Date()).toISOString()}: Error in fullfilling fileDefs: ${err.message} while processing ${message.id}`)

            // trigger rollback
            IndexingCores.forEach(core => {
              deleteDocs(core, genomeId)
            })

            when(updateHistory(message.id, 'error', err.message), () => {
              commit()
              processQueue(queue) // Resume
            }, (err) => {
              console.error(`${(new Date()).toISOString()}: Error in updating history: ${err} while processing ${message.id}`)
            })
          })
        } catch (err) {
          when(updateHistory(message.id, 'error', err.message), () => {
            commit()
            processQueue(queue) // Resume
          }, (err) => {
            console.error(`${(new Date()).toISOString()}: Error in updating history: ${err} while processing ${message.id}`)
          })
        }
      })
    }
  })
}

function updateJSONStreamWithAccessControl (fileName, filePath, core, owner) {
  const def = new Defer()

  try {
    let fileData = []
    const st = (new Date()).getTime()

    fs.createReadStream(filePath, {encoding: 'utf8'})
      .pipe(JSONStream.parse('*'))
      .pipe(es.mapSync(function (data) {
        fileData.push(data)
      }))
      .on('close', () => {
        const now = (new Date()).getTime()
        console.log(`  Reading ${fileName}(${filePath}) took ${(now - st)}ms, ${fileData.length} rows`)

        if (fileData.length === 0) {
          // console.log(`  Skipping Empty ${fileName}(${filePath})`);
          def.resolve({status: 'skip'})
          return def.promise
        }

        let processingErrors = false

        if (!publicCores.includes(core)) {
          fileData = fileData.map(item => {
            if (!item.public) { item.public = false }
            if (!item.owner) {
              item.owner = owner
            } else if (item.owner !== owner) {
              if (!item.user_write || (item.user_write && !item.user_write.includes(owner))) {
                processingErrors = `Item Owner ${item.owner} != Index User ${owner}`
              }
            }

            if (!item.user_read) { item.user_read = [] }
            if (!item.user_write) { item.user_write = [] }

            if (item._version_) {
              delete item._version_
            }
            return item
          })
        }

        if (!processingErrors) {
          // Update json file
          const outfile = fs.createWriteStream(filePath)
          es.readArray(fileData).pipe(JSONStream.stringify()).pipe(outfile)
            .on('close', () => {
              def.resolve({status: 'saved'})
            })
        } else {
          // Console.log(`Processing Errors in ${core} ${processingErrors}`);
          def.reject(processingErrors)
        }
      })
  } catch (err) {
    console.log(`Error updateJSONStreamWithAccessControl ${filePath}`)
    def.reject(err)
  }

  return def.promise
}

function updateHistory (id, state, msg) {
  const def = new Defer()
  const historyPath = Path.join(queueDirectory, 'history', id)

  console.log(`Updating History ${historyPath}`)

  fs.readJson(historyPath, (err, data) => {
    if (err) {
      def.reject(err)
      return def.promise
    }

    switch (state) {
      case 'submitted':
        data.state = state
        data.genomeId = msg
        data.submissionTime = new Date()
        break
      case 'error':
        data.state = state
        data.error = msg
        break
      default:
        data.state = state
        break
    }

    fs.writeJson(historyPath, data, (err) => {
      if (err) {
        console.log(`Error writing to history: ${id}`)
        def.reject(err)
        return def.promise
      }

      def.resolve(true)
    })
  })

  return def.promise
}
